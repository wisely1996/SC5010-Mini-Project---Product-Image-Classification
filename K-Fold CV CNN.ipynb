{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
    "from keras.models import Sequential \n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Flatten, Dense \n",
    "from keras import applications \n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import math \n",
    "import datetime\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14628\\3930989819.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_top.loc[:,\"class\"] = 0\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14628\\3930989819.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_btm.loc[:,\"class\"] = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9817</th>\n",
       "      <td>6ed9c2a655d6f84badb557bc210c1fb5.jpg</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>293c5038f681f017254800819a968212.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8238</th>\n",
       "      <td>90e1e6fbaa0c9a69ad0ee70bc2ea251d.jpg</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>42ea56b9983ba2bf7e01acf5dda172f5.jpg</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7671</th>\n",
       "      <td>f13b80836fbe7c635a9817e121093594.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9880</th>\n",
       "      <td>b61f5b0aec4375090b5a268a1f516ead.jpg</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>5072c6fb5ab3ae57524639805321a0f4.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>b2afaca110148bc0710f828c9d53b39f.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>3511390df751f1c8c70ca9c6897ddfc5.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9306</th>\n",
       "      <td>026f054380bff76064f26bcb2628ff40.jpg</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9094 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  filename  category\n",
       "9817  6ed9c2a655d6f84badb557bc210c1fb5.jpg        29\n",
       "234   293c5038f681f017254800819a968212.jpg         3\n",
       "8238  90e1e6fbaa0c9a69ad0ee70bc2ea251d.jpg        29\n",
       "2708  42ea56b9983ba2bf7e01acf5dda172f5.jpg        28\n",
       "7671  f13b80836fbe7c635a9817e121093594.jpg         4\n",
       "...                                    ...       ...\n",
       "9880  b61f5b0aec4375090b5a268a1f516ead.jpg        29\n",
       "2601  5072c6fb5ab3ae57524639805321a0f4.jpg         3\n",
       "2018  b2afaca110148bc0710f828c9d53b39f.jpg         3\n",
       "6834  3511390df751f1c8c70ca9c6897ddfc5.jpg         4\n",
       "9306  026f054380bff76064f26bcb2628ff40.jpg        29\n",
       "\n",
       "[9094 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df_top = df[(df[\"category\"] == 3)|(df[\"category\"] == 28)]\n",
    "df_top.loc[:,\"class\"] = 0\n",
    "df_btm = df[(df[\"category\"] == 4)|(df[\"category\"] == 29)]\n",
    "df_btm.loc[:,\"class\"] = 1\n",
    "\n",
    "df_cloth = pd.concat([df_top,df_btm]).reset_index(drop = True)\n",
    "df_cloth\n",
    "\n",
    "X = df_cloth[[\"filename\",\"category\"]]\n",
    "y = pd.DataFrame(df_cloth[\"class\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 5010)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7275, 2), (1819, 2), (7275, 1), (1819, 1))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = X_train[[\"filename\",\"category\"]]\n",
    "y1 = pd.DataFrame(y_train[\"class\"])\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.2, random_state = 5010)\n",
    "X1_train.shape, X1_test.shape, y1_train.shape, y1_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3803, 3472, 957, 862)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_path1 = \"train_dataset\"\n",
    "validation_path1 = \"validation_dataset\"\n",
    "\n",
    "len(os.listdir(train_path1 + '/top')), len(os.listdir(train_path1 + '/btm')), len(os.listdir(validation_path1 + '/top')), len(os.listdir(validation_path1 + '/btm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7275 images belonging to 2 classes.\n",
      "Found 1819 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (299, 299)\n",
    "shift = 0.2\n",
    "\n",
    "training_datagen1 = ImageDataGenerator(\n",
    "      rescale = 1./255,\n",
    "      rotation_range=5,\n",
    "      brightness_range=[0.2,1.5],\n",
    "      width_shift_range=shift, \n",
    "      height_shift_range=shift,\n",
    "      horizontal_flip=True)\n",
    "train_set1 = training_datagen1.flow_from_directory(\n",
    "     train_path1,\n",
    "     seed=9,\n",
    "\t   target_size=IMG_SIZE,\n",
    "     class_mode = \"categorical\")\n",
    "validation_datagen1 = ImageDataGenerator(\n",
    "      rescale = 1./255)\n",
    "val_set1 = validation_datagen1.flow_from_directory(\n",
    "\t  validation_path1,\n",
    "\t  target_size=IMG_SIZE,\n",
    "    shuffle=False,\n",
    "    seed=9,\n",
    "    class_mode = \"categorical\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 297, 297, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 148, 148, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 144, 144, 64)      51264     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 72, 72, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 68, 68, 128)       204928    \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 34, 34, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 16, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               16777728  \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,314,242\n",
      "Trainable params: 17,314,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "INIT_LR = 1e-4\n",
    "# Model 6 new data\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=IMG_SIZE+(3,)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2), \n",
    "    tf.keras.layers.Conv2D(64, (5,5), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (5,5), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax') \n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1011 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_path = \"test_dataset\"\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255)\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "\t  test_path,\n",
    "\t  target_size=IMG_SIZE,\n",
    "        shuffle=False,\n",
    "        seed=9,\n",
    "        class_mode = \"categorical\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rounded test labels [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.round(model.predict(train_set1),0)\n",
    "preds = np.round(model.predict(test_data),0)\n",
    "print(\"rounded test labels\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'btm': 0, 'top': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the class labels for the training data, in the original order \n",
    "train_labels = train_set1.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "train_labels = to_categorical(train_labels, num_classes=2)    \n",
    "\n",
    "# get the class labels for the training data, in the original order \n",
    "test_labels = test_data.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "test_labels = to_categorical(test_labels, num_classes=2)\n",
    "\n",
    "test_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#To get better visual of the confusion matrix:\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "   normalize=False,\n",
    "   title='Confusion matrix',\n",
    "   cmap=plt.cm.Blues):\n",
    " \n",
    "#Add Normalization Option\n",
    "\n",
    "   if normalize:\n",
    "     cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "     print('Normalized confusion matrix')\n",
    "   else:\n",
    "     print('Confusion matrix, without normalization')\n",
    " \n",
    "# print(cm)\n",
    " \n",
    "   plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "   plt.title(title)\n",
    "   plt.colorbar()\n",
    "   tick_marks = np.arange(len(classes))\n",
    "   plt.xticks(tick_marks, classes, rotation=45)\n",
    "   plt.yticks(tick_marks, classes)\n",
    " \n",
    "   fmt = '.2f' if normalize else 'd'\n",
    "   thresh = cm.max() / 2.\n",
    "   for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "      plt.text(j, i, format(cm[i, j], fmt), horizontalalignment='center', color='white' if cm[i, j] > thresh else 'black')\n",
    " \n",
    "   plt.tight_layout()\n",
    "   plt.ylabel('True label')\n",
    "   plt.xlabel('Predicted label') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8084 validated image filenames belonging to 2 classes.\n",
      "Found 2021 validated image filenames belonging to 2 classes.\n",
      "Model 0\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 599s 2s/step - loss: 0.5949 - accuracy: 0.6841 - val_loss: 0.5237 - val_accuracy: 0.7536\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 588s 2s/step - loss: 0.4665 - accuracy: 0.7887 - val_loss: 0.4562 - val_accuracy: 0.7956\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 583s 2s/step - loss: 0.4139 - accuracy: 0.8147 - val_loss: 0.3930 - val_accuracy: 0.8248\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 582s 2s/step - loss: 0.3777 - accuracy: 0.8347 - val_loss: 0.3877 - val_accuracy: 0.8337\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 590s 2s/step - loss: 0.3616 - accuracy: 0.8439 - val_loss: 0.3656 - val_accuracy: 0.8422\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 585s 2s/step - loss: 0.3424 - accuracy: 0.8535 - val_loss: 0.3488 - val_accuracy: 0.8516\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 584s 2s/step - loss: 0.3307 - accuracy: 0.8602 - val_loss: 0.3513 - val_accuracy: 0.8506\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 587s 2s/step - loss: 0.3201 - accuracy: 0.8647 - val_loss: 0.3436 - val_accuracy: 0.8540\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 598s 2s/step - loss: 0.3139 - accuracy: 0.8712 - val_loss: 0.3384 - val_accuracy: 0.8580\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 590s 2s/step - loss: 0.3075 - accuracy: 0.8709 - val_loss: 0.3193 - val_accuracy: 0.8619\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 588s 2s/step - loss: 0.2960 - accuracy: 0.8758 - val_loss: 0.3532 - val_accuracy: 0.8550\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 590s 2s/step - loss: 0.2928 - accuracy: 0.8800 - val_loss: 0.3241 - val_accuracy: 0.8595\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 591s 2s/step - loss: 0.2852 - accuracy: 0.8846 - val_loss: 0.3333 - val_accuracy: 0.8545\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 592s 2s/step - loss: 0.2819 - accuracy: 0.8833 - val_loss: 0.3122 - val_accuracy: 0.8664\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 593s 2s/step - loss: 0.2800 - accuracy: 0.8827 - val_loss: 0.3054 - val_accuracy: 0.8743\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 567s 2s/step - loss: 0.2679 - accuracy: 0.8874 - val_loss: 0.2955 - val_accuracy: 0.8808\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2659 - accuracy: 0.8908 - val_loss: 0.3012 - val_accuracy: 0.8738\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.2544 - accuracy: 0.8988 - val_loss: 0.2889 - val_accuracy: 0.8847\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.2572 - accuracy: 0.8983 - val_loss: 0.2923 - val_accuracy: 0.8788\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.2469 - accuracy: 0.8988 - val_loss: 0.2935 - val_accuracy: 0.8812\n",
      "Found 8084 validated image filenames belonging to 2 classes.\n",
      "Found 2021 validated image filenames belonging to 2 classes.\n",
      "Model 1\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 558s 2s/step - loss: 0.5952 - accuracy: 0.6904 - val_loss: 0.5233 - val_accuracy: 0.7600\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.4956 - accuracy: 0.7697 - val_loss: 0.4526 - val_accuracy: 0.7892\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.4341 - accuracy: 0.8039 - val_loss: 0.4363 - val_accuracy: 0.8046\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.4065 - accuracy: 0.8199 - val_loss: 0.4226 - val_accuracy: 0.7942\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.3832 - accuracy: 0.8289 - val_loss: 0.3550 - val_accuracy: 0.8481\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.3651 - accuracy: 0.8414 - val_loss: 0.3657 - val_accuracy: 0.8446\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.3496 - accuracy: 0.8527 - val_loss: 0.3598 - val_accuracy: 0.8496\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3384 - accuracy: 0.8551 - val_loss: 0.3203 - val_accuracy: 0.8664\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3138 - accuracy: 0.8699 - val_loss: 0.3147 - val_accuracy: 0.8619\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.3146 - accuracy: 0.8725 - val_loss: 0.3046 - val_accuracy: 0.8803\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3070 - accuracy: 0.8670 - val_loss: 0.3368 - val_accuracy: 0.8629\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2907 - accuracy: 0.8824 - val_loss: 0.2978 - val_accuracy: 0.8743\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2872 - accuracy: 0.8833 - val_loss: 0.2978 - val_accuracy: 0.8847\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.2874 - accuracy: 0.8800 - val_loss: 0.2876 - val_accuracy: 0.8817\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2799 - accuracy: 0.8869 - val_loss: 0.2850 - val_accuracy: 0.8812\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2674 - accuracy: 0.8913 - val_loss: 0.2842 - val_accuracy: 0.8827\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.2637 - accuracy: 0.8936 - val_loss: 0.2846 - val_accuracy: 0.8887\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.2603 - accuracy: 0.8972 - val_loss: 0.2802 - val_accuracy: 0.8852\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2558 - accuracy: 0.8976 - val_loss: 0.2830 - val_accuracy: 0.8926\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2456 - accuracy: 0.9050 - val_loss: 0.2725 - val_accuracy: 0.8936\n",
      "Found 8084 validated image filenames belonging to 2 classes.\n",
      "Found 2021 validated image filenames belonging to 2 classes.\n",
      "Model 2\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 558s 2s/step - loss: 0.5879 - accuracy: 0.6969 - val_loss: 0.5093 - val_accuracy: 0.7590\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.4684 - accuracy: 0.7864 - val_loss: 0.4706 - val_accuracy: 0.7729\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.4086 - accuracy: 0.8199 - val_loss: 0.3573 - val_accuracy: 0.8590\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.3822 - accuracy: 0.8307 - val_loss: 0.4755 - val_accuracy: 0.8026\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.3731 - accuracy: 0.8381 - val_loss: 0.3443 - val_accuracy: 0.8491\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3474 - accuracy: 0.8496 - val_loss: 0.3245 - val_accuracy: 0.8664\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3420 - accuracy: 0.8561 - val_loss: 0.3112 - val_accuracy: 0.8738\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 556s 2s/step - loss: 0.3343 - accuracy: 0.8594 - val_loss: 0.3025 - val_accuracy: 0.8837\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3214 - accuracy: 0.8611 - val_loss: 0.2914 - val_accuracy: 0.8872\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3090 - accuracy: 0.8739 - val_loss: 0.3120 - val_accuracy: 0.8817\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3057 - accuracy: 0.8727 - val_loss: 0.2955 - val_accuracy: 0.8758\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 558s 2s/step - loss: 0.2943 - accuracy: 0.8790 - val_loss: 0.2810 - val_accuracy: 0.8916\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2879 - accuracy: 0.8830 - val_loss: 0.2812 - val_accuracy: 0.8916\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2825 - accuracy: 0.8847 - val_loss: 0.2669 - val_accuracy: 0.8926\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2697 - accuracy: 0.8893 - val_loss: 0.3176 - val_accuracy: 0.8743\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2672 - accuracy: 0.8960 - val_loss: 0.2699 - val_accuracy: 0.8877\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2668 - accuracy: 0.8911 - val_loss: 0.2590 - val_accuracy: 0.8976\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2528 - accuracy: 0.8991 - val_loss: 0.2922 - val_accuracy: 0.8748\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2574 - accuracy: 0.8953 - val_loss: 0.2594 - val_accuracy: 0.8961\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2490 - accuracy: 0.9004 - val_loss: 0.2575 - val_accuracy: 0.9055\n",
      "Found 8084 validated image filenames belonging to 2 classes.\n",
      "Found 2021 validated image filenames belonging to 2 classes.\n",
      "Model 3\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 558s 2s/step - loss: 0.5923 - accuracy: 0.6922 - val_loss: 0.5782 - val_accuracy: 0.6922\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.4983 - accuracy: 0.7700 - val_loss: 0.4609 - val_accuracy: 0.8006\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.4347 - accuracy: 0.8119 - val_loss: 0.4640 - val_accuracy: 0.7902\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3913 - accuracy: 0.8325 - val_loss: 0.3923 - val_accuracy: 0.8268\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3708 - accuracy: 0.8422 - val_loss: 0.3629 - val_accuracy: 0.8461\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3522 - accuracy: 0.8530 - val_loss: 0.3604 - val_accuracy: 0.8402\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3400 - accuracy: 0.8551 - val_loss: 0.3510 - val_accuracy: 0.8501\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3266 - accuracy: 0.8606 - val_loss: 0.3569 - val_accuracy: 0.8555\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3243 - accuracy: 0.8616 - val_loss: 0.3310 - val_accuracy: 0.8615\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3071 - accuracy: 0.8720 - val_loss: 0.3682 - val_accuracy: 0.8511\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3009 - accuracy: 0.8746 - val_loss: 0.3322 - val_accuracy: 0.8585\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2969 - accuracy: 0.8790 - val_loss: 0.3166 - val_accuracy: 0.8634\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2866 - accuracy: 0.8841 - val_loss: 0.3046 - val_accuracy: 0.8684\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2805 - accuracy: 0.8848 - val_loss: 0.3400 - val_accuracy: 0.8674\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2747 - accuracy: 0.8916 - val_loss: 0.3059 - val_accuracy: 0.8753\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2797 - accuracy: 0.8878 - val_loss: 0.2898 - val_accuracy: 0.8867\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2574 - accuracy: 0.9003 - val_loss: 0.2897 - val_accuracy: 0.8857\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2565 - accuracy: 0.8975 - val_loss: 0.2865 - val_accuracy: 0.8852\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2516 - accuracy: 0.9004 - val_loss: 0.2800 - val_accuracy: 0.8832\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2484 - accuracy: 0.8979 - val_loss: 0.2925 - val_accuracy: 0.8862\n",
      "Found 8084 validated image filenames belonging to 2 classes.\n",
      "Found 2021 validated image filenames belonging to 2 classes.\n",
      "Model 4\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 558s 2s/step - loss: 0.5923 - accuracy: 0.6878 - val_loss: 0.5201 - val_accuracy: 0.7674\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.4898 - accuracy: 0.7713 - val_loss: 0.4328 - val_accuracy: 0.8140\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.4387 - accuracy: 0.8005 - val_loss: 0.3989 - val_accuracy: 0.8189\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3956 - accuracy: 0.8263 - val_loss: 0.3820 - val_accuracy: 0.8402\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3680 - accuracy: 0.8417 - val_loss: 0.3593 - val_accuracy: 0.8491\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3525 - accuracy: 0.8478 - val_loss: 0.3575 - val_accuracy: 0.8456\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3443 - accuracy: 0.8554 - val_loss: 0.3219 - val_accuracy: 0.8644\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3276 - accuracy: 0.8618 - val_loss: 0.3354 - val_accuracy: 0.8560\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3166 - accuracy: 0.8632 - val_loss: 0.3203 - val_accuracy: 0.8699\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3130 - accuracy: 0.8679 - val_loss: 0.3190 - val_accuracy: 0.8669\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.3017 - accuracy: 0.8746 - val_loss: 0.3248 - val_accuracy: 0.8619\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2999 - accuracy: 0.8742 - val_loss: 0.2841 - val_accuracy: 0.8852\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2894 - accuracy: 0.8805 - val_loss: 0.2903 - val_accuracy: 0.8773\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2864 - accuracy: 0.8822 - val_loss: 0.2915 - val_accuracy: 0.8758\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2769 - accuracy: 0.8859 - val_loss: 0.2868 - val_accuracy: 0.8862\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2647 - accuracy: 0.8911 - val_loss: 0.2987 - val_accuracy: 0.8852\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2659 - accuracy: 0.8941 - val_loss: 0.2653 - val_accuracy: 0.8936\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 558s 2s/step - loss: 0.2610 - accuracy: 0.8914 - val_loss: 0.2769 - val_accuracy: 0.8946\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 558s 2s/step - loss: 0.2599 - accuracy: 0.8957 - val_loss: 0.2615 - val_accuracy: 0.8986\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 557s 2s/step - loss: 0.2506 - accuracy: 0.8996 - val_loss: 0.2625 - val_accuracy: 0.8906\n",
      "Finished training Model 9\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "SPLITS = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits = SPLITS, random_state = 7, shuffle = True)\n",
    "df_cloth[\"class\"] = df_cloth[\"class\"].astype(\"str\")\n",
    "df_cloth[\"category\"] = df_cloth[\"category\"].astype(\"str\")\n",
    "df_cloth[\"category\"] = df_cloth[\"category\"].apply(lambda r: \"0\" + r if len(r) == 1 else r)\n",
    "df_cloth[\"filepath\"] = df_cloth[\"category\"] + '/' + df_cloth[\"filename\"]\n",
    "\n",
    "X = df_cloth\n",
    "Y = df_cloth[[\"class\"]]\n",
    "\n",
    "skf_model_list = [0]*SPLITS\n",
    "\n",
    "training_datagen_skf = ImageDataGenerator(\n",
    "      rescale = 1./255,\n",
    "      rotation_range=5,\n",
    "      brightness_range=[0.2,1.5],\n",
    "      width_shift_range=shift, \n",
    "      height_shift_range=shift,\n",
    "      horizontal_flip=True)\n",
    "count = 0\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "      training_data = X.iloc[train_index]\n",
    "      validation_data = X.iloc[test_index]\n",
    "\n",
    "      train_iterator = training_datagen_skf.flow_from_dataframe(training_data,\n",
    "                                                        x_col='filepath',\n",
    "                                                        y_col='class',\n",
    "                                                        directory='./train',\n",
    "                                                        target_size=IMG_SIZE,\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        shuffle=True)\n",
    "\n",
    "      validation_iterator = training_datagen_skf.flow_from_dataframe(validation_data,\n",
    "                                                x_col='filepath',\n",
    "                                                y_col='class',\n",
    "                                                directory='./train',\n",
    "                                                target_size=IMG_SIZE,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=True)\n",
    "      print(f\"Model {count}\")\n",
    "\n",
    "      skf_model_list[count] = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=IMG_SIZE+(3,)),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2), \n",
    "            tf.keras.layers.Conv2D(64, (5,5), activation='relu'), \n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Conv2D(128, (5,5), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(2, activation='softmax') \n",
    "            ])\n",
    "      skf_model_list[count].compile(\n",
    "            optimizer= tf.keras.optimizers.Adam(\n",
    "            learning_rate=INIT_LR, \n",
    "            decay=INIT_LR / EPOCHS),\n",
    "            loss=keras.losses.categorical_crossentropy,\n",
    "            metrics=[\"accuracy\"]\n",
    "            )\n",
    "      H1 = skf_model_list[count].fit(\n",
    "            train_iterator, \n",
    "            epochs=20, \n",
    "            validation_data=validation_iterator)\n",
    "\n",
    "      count += 1\n",
    "print(f\"Finished training Model {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(SPLITS):\n",
    "    fpath = f\"skf_{j}.h5\"\n",
    "    skf_model_list[j].save(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         top       0.97      0.90      0.93       507\n",
      "         btm       0.90      0.98      0.94       504\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1011\n",
      "   macro avg       0.94      0.94      0.94      1011\n",
      "weighted avg       0.94      0.94      0.94      1011\n",
      " samples avg       0.94      0.94      0.94      1011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificationlabels = ['btm','top']\n",
    "SPLITS = 5\n",
    "\n",
    "skf_model_list = [0]*SPLITS\n",
    "for j in range(SPLITS):\n",
    "    skf_model_list[j] = keras.models.load_model(f'skf_{j}.h5')\n",
    "\n",
    "results1 = np.zeros( (X_test.shape[0],2) ) \n",
    "for j in range(SPLITS):\n",
    "    results1 = results1 + skf_model_list[j].predict(test_data)\n",
    "\n",
    "results1 = np.round(results1,0)\n",
    "\n",
    "final_results1 = np.zeros( (X_test.shape[0],2) )\n",
    "final_results1[np.arange(results1.shape[0]),np.argmin(results1, axis = 1)] = 1\n",
    "\n",
    "classificationlabels = ['top','btm']\n",
    "classification_metrics = metrics.classification_report(test_labels, final_results1, target_names=classificationlabels)\n",
    "\n",
    "print(classification_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical_test_labels = pd.DataFrame(test_labels).idxmax(axis=1)\n",
    "categorical_preds3 = pd.DataFrame(final_results1).idxmax(axis=1)\n",
    "confusion_matrix4 = confusion_matrix(categorical_test_labels, categorical_preds3)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix4, ['top','btm'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aecd030d4c8316a52bf122072e28f84bcc79844c2684e041fef2e3f1d9f59078"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
